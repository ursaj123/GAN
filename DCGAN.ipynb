{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c709f74e",
   "metadata": {},
   "source": [
    "## *NECESSARY IMPORTS* #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d95a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import  transforms,datasets \n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6470169",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea5794",
   "metadata": {},
   "source": [
    "## *DATA IMPORTS AND DEFINING THE DATALOADERS* #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936fff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(300),transforms.CenterCrop(64),transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc9f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "dataset = datasets.ImageFolder(data_dir,transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99905dc4",
   "metadata": {},
   "source": [
    "## *DISPLAYING THE IMAGES*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf7f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (16,16)\n",
    "def PlotBatch(dataloader):  # prints the image of a batch in dataloader\n",
    "  sample_data = next(iter(dataloader))[0].to(device)\n",
    "  plt.figure(figsize=figsize)\n",
    "  plt.axis('off')  # it is written not to print axis, instead it will make a white border around the images\n",
    "  plt.title(\"IMAGES\")\n",
    "  plt.imshow(np.transpose(torchvision.utils.make_grid(\n",
    "      sample_data, normalize = False\n",
    "  ).cpu(), (1,2,0))) # (1,2,0) is for first showing the images correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffee6545",
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotBatch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6655988d",
   "metadata": {},
   "source": [
    "## *DEFINING THE NETWORKS* \n",
    "### * *GENERATOR NETWORK* \n",
    "### * *DISCRIMINATOR NETWORK* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7815d5",
   "metadata": {},
   "source": [
    "## *GENERATOR NETWORK* #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb374f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # input will be a noise matrix array of (batch_size,100) \n",
    "        self.linear = nn.Linear(100,512*16).to(device)\n",
    "        # input_shape is (512,4,4)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            # input_shape is (512,4,4)\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            # input_shape is (256,8,8)\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            # input_shape is (128,16,16)\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            # input_shape is (64,32,32)\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "            # output_shape is (3,64,64)\n",
    "        ).to(device)\n",
    "        \n",
    "    def forward(self, x): # x is noise of size (batch_size,100) \n",
    "        op = self.linear(x)\n",
    "        op = op.reshape(-1,512,4,4)\n",
    "        op = self.model(op)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d6cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().to(device)\n",
    "summary(generator, (100,)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fee2bf",
   "metadata": {},
   "source": [
    "## *DISCRIMINATOR NETWORK* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # image_shape is (3,64,64)\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=4, stride=2, padding=1), \n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # image_shape is (16,32,32)\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # image_shape is (32,16,16)\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # image_shape is (64,8,8)\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # image_shape is (128,4,4)\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            #image_shape is (256,2,2)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "            #output_shape is (512,1,1)\n",
    "            \n",
    "        ).to(device)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        op = self.model(x)\n",
    "        op = op.reshape(-1,512)\n",
    "        op = self.classifier(op)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0580768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator().to(device)\n",
    "summary(discriminator,(3,64,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ef3ca",
   "metadata": {},
   "source": [
    "## *HYPERPARAMETERS, LOSS FUNCTION AND OPTIMIZER*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9545746",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-4\n",
    "batch_size = 16\n",
    "epochs = 2\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "gen_optim = optim.Adam(generator.parameters(), lr=lr)\n",
    "dis_optim = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d5ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    discriminator_loss = []\n",
    "    generator_loss = []\n",
    "    for epoch in range(1,epochs+1):\n",
    "        total_dis_loss = 0\n",
    "        total_gen_loss = 0\n",
    "        for i, batch in enumerate(train_loader,1):\n",
    "            # training the discrminator model\n",
    "            target_for_true = torch.ones(batch[0].shape[0]).to(device)\n",
    "            target_for_fake = torch.zeros(batch[0].shape[0]).to(device)\n",
    "            # passing the true data and fake data through the discriminator model with their actual targets as real or fake    \n",
    "            # forward propoagation\n",
    "            model_input_for_true_data = (batch[0]/255.0).to(device)\n",
    "            model_output_for_true_data = discriminator(model_input_for_true_data)\n",
    "            model_input_for_fake_data = torch.randn(batch[0].shape[0],100).to(device)\n",
    "            model_output_for_fake_data = discriminator(generator(model_input_for_fake_data))\n",
    "            model_output_for_true_data = model_output_for_true_data.reshape(batch[0].shape[0])\n",
    "            model_output_for_fake_data = model_output_for_fake_data.reshape(batch[0].shape[0])\n",
    "            loss_of_true_data = criterion(model_output_for_true_data, target_for_true)\n",
    "            loss_of_fake_data = criterion(model_output_for_fake_data, target_for_fake)\n",
    "            loss_of_dis_model = (loss_of_true_data+loss_of_fake_data)/2\n",
    "            total_dis_loss+= (loss_of_dis_model).item()\n",
    "            # backpropoagation\n",
    "            dis_optim.zero_grad()\n",
    "            loss_of_dis_model.backward()\n",
    "            dis_optim.step()\n",
    "            \n",
    "            \n",
    "            # now the fake data will be passed through the generator model as having its target as real\n",
    "            # forward propoagation\n",
    "            model_input_for_fake_data2 = torch.randn(batch[0].shape[0],100).to(device)\n",
    "            model_output_for_fake_data2 = discriminator(generator(model_input_for_fake_data2))\n",
    "            model_output_for_fake_data2 = model_output_for_fake_data2.reshape(batch[0].shape[0])\n",
    "            target_for_true2 = torch.ones(batch[0].shape[0]).to(device)\n",
    "            loss_of_gen_model = criterion(model_output_for_fake_data2,target_for_true2)\n",
    "            total_gen_loss+= (loss_of_gen_model).item()\n",
    "            # backward propoagation\n",
    "            gen_optim.zero_grad()\n",
    "            loss_of_gen_model.backward()\n",
    "            gen_optim.step()\n",
    "            \n",
    "            if i%100==0:\n",
    "                with torch.no_grad():\n",
    "                    noise = torch.randn(1,100).to(device)\n",
    "                    noise = generator(noise)\n",
    "                    noise = noise.squeeze(0)\n",
    "                    noise = noise.detach().cpu()\n",
    "                    plt.imshow(np.transpose(noise.cpu(), (1,2,0)))\n",
    "                    plt.show()\n",
    "        \n",
    "        discriminator_loss.append(total_dis_loss/len(train_loader))\n",
    "        generator_loss.append(total_gen_loss/len(train_loader))\n",
    "        print(\"[%d/%d], dis_loss = %f, gen_loss = %f\" % (epoch,epochs,discriminator_loss[-1], generator_loss[-1]))                           \n",
    "    return discriminator_loss, generator_loss     \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10da1c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dis_loss, gen_loss = train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a501d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, epochs+1), dis_loss, label = \"Discriminator loss\")\n",
    "plt.plot(range(1, epochs+1), gen_loss, label = \"Generator loss\")\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"dis_loss vs gen_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf9e2e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
